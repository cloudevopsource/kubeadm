
# 02.初始化master节点

<!-- TOC -->

- [01.安裝前的準備](#01.安裝前的準備)
    - [在每個節點安裝依賴工具](#在每個節點安裝依賴工具)
    - [安裝及準備ansible(在部署主機安裝ansible控制端)](#安裝及準備ansible(在部署主機安裝ansible控制端))

<!-- /TOC -->

## 创建kubeadm config配置文件 (第一个master节点)


``` bash
cd /stage
rm -f ./kubeadm-config.yaml
cat <<EOF > ./kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.7
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "127.0.0.1:8443"
networking:
  serviceSubnet: "10.96.0.0/16"
  podSubnet: "10.100.0.1/16"
  dnsDomain: "cluster.local"
EOF
```
## 初始化第一个master节点

+ kubeadm init
``` bash
# 根据您服务器网速的情况，您需要等候 3 - 10 分钟
kubeadm init --config=kubeadm-config.yaml --upload-certs

[root@k8scloud1 stage]# kubeadm init --config=kubeadm-config.yaml --upload-certs
[init] Using Kubernetes version: v1.16.7
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8scloud1.frcloud.io kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.22.11 127.0.0.1]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8scloud1.frcloud.io localhost] and IPs [172.16.22.11 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8scloud1.frcloud.io localhost] and IPs [172.16.22.11 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "admin.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 33.516108 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.16" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
3d777df18b69cd28833715d6494358f408ca80d35776301b2451e333a2864ad0
[mark-control-plane] Marking the node k8scloud1.frcloud.io as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8scloud1.frcloud.io as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: nmvt6o.fmb234ffc65epoc7
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 127.0.0.1:8443 --token nmvt6o.fmb234ffc65epoc7 \
    --discovery-token-ca-cert-hash sha256:30e80c75f6da333b57b22415579eb1765c93c5461b2b0a127d8e45dc19e0f670 \
    --control-plane --certificate-key 3d777df18b69cd28833715d6494358f408ca80d35776301b2451e333a2864ad0

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use 
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 127.0.0.1:8443 --token nmvt6o.fmb234ffc65epoc7 \
    --discovery-token-ca-cert-hash sha256:30e80c75f6da333b57b22415579eb1765c93c5461b2b0a127d8e45dc19e0f670
```

+ 配置 kubeconfig
``` bash
rm -rf /root/.kube/
mkdir /root/.kube/
cp -i /etc/kubernetes/admin.conf /root/.kube/config
```
+ 执行结果

``` bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 127.0.0.1:8443 --token ziyp1y.zc58k1t6tsnes4dh \
    --discovery-token-ca-cert-hash sha256:63fff70eeb450678753e4a82761f09c864edc73d0b14efd30fff38673b6d7c64 \
    --control-plane --certificate-key 41b2b761f5d0c4a9af1060f1b9edf14b687b6dd572e60289aa4fc150fb62df47

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 127.0.0.1:8443 --token ziyp1y.zc58k1t6tsnes4dh \
    --discovery-token-ca-cert-hash sha256:63fff70eeb450678753e4a82761f09c864edc73d0b14efd30fff38673b6d7c64
```
+ 配置kubeconfig
``` bash
 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config
  ```

## 安装 calico 网络插件
``` bash
# 参考文档 https://docs.projectcalico.org/v3.9/getting-started/kubernetes/
cd /stage
rm -f calico-3.9.2.yaml
wget https://kuboard.cn/install-script/calico/calico-3.9.2.yaml
sed -i "s#192\.168\.0\.0/16#10\.100\.0\.1/16#" calico-3.9.2.yaml
kubectl apply -f calico-3.9.2.yaml

 注意：COREDNS POD会等待CALICO POD 执行完成
```

## 检查 master 初始化结果(只在第一个 master 节点执行)
``` bash
# 查看 master 节点初始化结果
[root@k8scloud1 stage]# kubectl get nodes
NAME                   STATUS   ROLES    AGE   VERSION
k8scloud1.frcloud.io   Ready    master   25m   v1.16.4

```

## 检查pod运行状态
``` bash
# 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态，大约有9个容器

[root@k8scloud1 stage]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                           READY   STATUS    RESTARTS   AGE
kube-system   calico-kube-controllers-dc6cb64cb-bdd88        1/1     Running   0          6m52s
kube-system   calico-node-mc6j5                              1/1     Running   0          6m53s
kube-system   coredns-67c766df46-62br6                       1/1     Running   0          17m
kube-system   coredns-67c766df46-pbbz2                       1/1     Running   0          17m
kube-system   etcd-k8scloud1.frcloud.io                      1/1     Running   0          16m
kube-system   kube-apiserver-k8scloud1.frcloud.io            1/1     Running   0          16m
kube-system   kube-controller-manager-k8scloud1.frcloud.io   1/1     Running   0          16m
kube-system   kube-proxy-c5xdl                               1/1     Running   0          17m
kube-system   kube-scheduler-k8scloud1.frcloud.io            1/1     Running   0          16m


```
## 初始化第二、三个master节点(初始化 master 节点的 token 有效时间为 2 小时)

+ 分别在二、三个master节点root帐户执行
``` bash
kubeadm join 127.0.0.1:8443 --token ziyp1y.zc58k1t6tsnes4dh \
    --discovery-token-ca-cert-hash sha256:63fff70eeb450678753e4a82761f09c864edc73d0b14efd30fff38673b6d7c64 \
    --control-plane --certificate-key 41b2b761f5d0c4a9af1060f1b9edf14b687b6dd572e60289aa4fc150fb62df47
```

+ 配置kubeconfig
``` bash
 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config
 ```

## 初始化第二、三个master节点(若初始化 master 节点的 token 大于 2 小时)
若超过两小时需重新获取token码
+ 获得 certificate key
在 k8scloud1 上执行
```bash
# 只在 第一个master节点 k8scloud1 上执行
kubeadm init phase upload-certs --upload-certs
```
+ 输出结果如下：
```bash
[root@k8scloud1 stage]# kubeadm init phase upload-certs --upload-certs
W1216 18:24:17.070675   22089 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1216 18:24:17.070723   22089 version.go:102] falling back to the local client version: v1.16.4
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
142efbce46095c1e7c274295df6d7407d29a7b115a4d78977873ea75c8474747
```

+ 获得 join 命令
```bash
# 只在 第一个master节点 k8scloud1 上执行
kubeadm token create --print-join-command
```

+ 输出结果如下：
```bash
[root@k8scloud1 stage]# kubeadm token create --print-join-command
kubeadm join 127.0.0.1:8443 --token sxtuuk.sml6a8nst9csqtgw     --discovery-token-ca-cert-hash sha256:63fff70eeb450678753e4a82761f09c864edc73d0b14efd30fff38673b6d7c64
```




